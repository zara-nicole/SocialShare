{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 14 09:26:22 2018\n",
    "\n",
    "@author: zsaldanh\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import functions_social as fs\n",
    "        \n",
    "df_all = pd.DataFrame()\n",
    "my_data = {}\n",
    "list_names = []\n",
    "for filename in glob.glob('trafficsources/*'):\n",
    "    #creates dictionary of all the dataframes of the csv files where sites with at least one nan is dropped\n",
    "    data = pd.read_csv(filename)\n",
    "    data = fs.split_month(data).iloc[:,0:4]\n",
    "    site_list = ['pinterest','facebook','amazon','google','twitter','outlook','linkedin','reddit',\n",
    "                 'youtube', 'whatsapp','instagram','stackexchange','slideshare','vk','getpocket','stackoverflow','quora',\n",
    "                'netvibes','soundcloud','wikia','digg','vimeo','meetup']\n",
    "    #?print top 6 of each before pivoting and dropping nans, compare to final list\n",
    "    #maybe create list of 'honorable mentions'\n",
    "    data = fs.convert_channel(data,site_list)\n",
    "    data = data.groupby(['year_month','channel']).sum().reset_index().sort_values(by=['year_month','share'],ascending=False)\n",
    "    data = data.pivot('channel','year_month','share')\n",
    "    #sum_list = fs.compute_sum(data)\n",
    "    #other_list = [1-x for x in sum_list]\n",
    "    #data.loc['other'] = other_list\n",
    "    name = filename[15:(filename[15:-4].index('.'))+15] + '_df'\n",
    "        \n",
    "    if name not in list_names:\n",
    "        my_data[name] = data\n",
    "        list_names.append(name)\n",
    "    else:\n",
    "        my_data[filename[15:-4]+'_df'] = data\n",
    "        list_names.append(filename[15:-4]+'_df')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'my_data_nan' (dict)\n"
     ]
    }
   ],
   "source": [
    "my_data_nan = my_data.copy()\n",
    "len(my_data)\n",
    "%store my_data_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in the facebook dataset we dropped all nan values which would have taken out the websited that were\n",
    "#not in the dataset during certain months\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
